AWSTemplateFormatVersion: "2010-09-09"
Description: Deploy a cluster managed with Slurm

Parameters:
  SlurmPackageUrl:
    Type: String
    Default: https://download.schedmd.com/slurm/slurm-22.05-latest.tar.bz2
    Description: URL to the Slurm installation package. The filename must be like slurm-*.tar.bz2

  PointCloudUrl:
    Type: String
    Description: Point cloud processing algorithm URL
    Default: https://api.github.com/repos/GarciaBarreiro/octree-mpi/tarball

  ArmadilloUrl:
    Type: String
    Description: Armadillo package URL
    Default: https://sourceforge.net/projects/arma/files/armadillo-14.2.2.tar.xz

  VpcId:
    Type: AWS::EC2::VPC::Id
    Description: VPC where the cluster nodes will be launched

  HeadNodeAZ:
    Type: AWS::EC2::AvailabilityZone::Name
    Description: Availability Zone where the head node will be launched

  KeyPair:
    Type: AWS::EC2::KeyPair::KeyName
    Description: Key pair that will be used to launch the cluster instances

  LatestAmiId:
    Type: AWS::SSM::Parameter::Value<AWS::EC2::Image::Id>
    Default: /aws/service/ami-amazon-linux-latest/al2023-ami-kernel-6.1-x86_64

  HeadNodeInstanceType:
    Type: String
    Default: c5.large
    Description: Instance type of the head node

  ComputeNodeInstanceType:
    Type: String
    Default: c5.large
    Description: Instance type of the compute nodes

  ComputeNodesAmount:
    Type: Number
    Default: 2
    AllowedValues: [1, 2, 3, 4, 5, 6, 7, 8]
    Description: "Number of compute nodes (min: 1, max: 8)"

Metadata: 
  AWS::CloudFormation::Interface: 
    ParameterGroups:
      - Label: 
          default: Network
        Parameters:
          - VpcId
          - HeadNodeAZ
      - Label:
          default: Instances
        Parameters:
          - HeadNodeInstanceType
          - ComputeNodeInstanceType
          - ComputeNodesAmount
          - KeyPair
          - LatestAmiId
      - Label: 
          default: Packages
        Parameters: 
          - SlurmPackageUrl
          - PluginPrefixUrl
          - PointCloudUrl
          - ArmadilloUrl
    ParameterLabels:
      VpcId: 
        default: VPC ID
      HeadNodeAZ:
        default: Head node AZ
      HeadNodeInstanceType:
        default: Head node instance type
      ComputeNodeInstanceType: 
        default: Compute node instance type
      ComputeNodesAmount:
        default: Number of compute nodes
      KeyPair: 
        default: Key pair
      LatestAmiId: 
        default: Latest Amazon Linux 2 AMI ID
      SlurmPackageUrl: 
        default: Slurm package URL
      PointCloudUrl:
        default: Point cloud processing algorithm URL
      ArmadilloUrl:
        default: Armadillo package URL
      PluginPrefixUrl: 
        default: Plugin URL prefix

Transform: AWS::LanguageExtensions

Mappings:
  ComputeNodesData:
    Identifiers:
      "1": ["0"]
      "2": ["0", "1"]
      "3": ["0", "1", "2"]
      "4": ["0", "1", "2", "3"]
      "5": ["0", "1", "2", "3", "4"]
      "6": ["0", "1", "2", "3", "4", "5"]
      "7": ["0", "1", "2", "3", "4", "5", "6"]
      "8": ["0", "1", "2", "3", "4", "5", "6", "7"]

Resources:
  SecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
        GroupDescription: Allow SSH traffic from Internet and traffic between Slurm nodes
        VpcId: !Ref VpcId
        SecurityGroupIngress:
        - IpProtocol: tcp
          FromPort: 22
          ToPort: 22
          CidrIp: 0.0.0.0/0

  SecurityGroupInbound:
      Type: AWS::EC2::SecurityGroupIngress
      Properties:
        IpProtocol: -1
        SourceSecurityGroupId: !GetAtt [ SecurityGroup, GroupId ]
        GroupId: !GetAtt [ SecurityGroup, GroupId ]
          
  'Fn::ForEach::ComputeNodes':
    - InstanceID
    - !FindInMap [ComputeNodesData, Identifiers, !Ref ComputeNodesAmount]
    - 'ComputeNode${InstanceID}':
        Type: AWS::EC2::Instance
        DependsOn: HeadNode
        Properties:
          ImageId: !Ref LatestAmiId
          InstanceType: !Ref ComputeNodeInstanceType
          IamInstanceProfile: LabInstanceProfile
          KeyName: !Ref KeyPair
          SecurityGroupIds:
            - !GetAtt [SecurityGroup, GroupId]
          AvailabilityZone: !Ref HeadNodeAZ
          Tags:
            - Key: Name
              Value: !Join [ "-", [ !Ref AWS::StackName, !Join [ "", [ "cn", !Ref InstanceID ]]]]
          UserData:
            Fn::Base64: !Sub |
                #!/bin/bash -x
                # Install packages
                # yum update -y
                # amazon-linux-extras install epel -y
                yum install munge munge-libs munge-devel -y
                yum install openssl openssl-devel pam-devel numactl numactl-devel hwloc hwloc-devel lua lua-devel readline-devel rrdtool-devel ncurses-devel libibmad libibumad rpm-build pmix-devel -y

                yum install dbus-devel -y

                # Configure Munge
                echo "welcometoslurmamazonuserwelcometoslurmamazonuserwelcometoslurmamazonuser" | tee /etc/munge/munge.key
                chown munge:munge /etc/munge/munge.key
                chmod 600 /etc/munge/munge.key
                chown -R munge /etc/munge/ /var/log/munge/
                chmod 0700 /etc/munge/ /var/log/munge/
                systemctl enable munge
                systemctl start munge
                sleep 15

                # Install OpenMPI
                yum install openmpi openmpi-devel eigen3 -y

                # Mount NFS share
                mkdir -p /nfs
                mount -t nfs ${HeadNode.PrivateIp}:/nfs /nfs
                echo "${HeadNode.PrivateIp}:/nfs /nfs nfs rw,nosuid 0 0" >> /etc/fstab
                export SLURM_HOME=/nfs/slurm

                # Set environment variables
                echo 'export SLURM_HOME=/nfs/slurm' | tee /etc/profile.d/slurm.sh
                echo 'export SLURM_CONF=$SLURM_HOME/etc/slurm.conf' | tee -a /etc/profile.d/slurm.sh
                echo 'export SLURM_NODENAME=cn${InstanceID}' | tee -a /etc/profile.d/slurm.sh
                echo 'export PATH=/nfs/slurm/bin:$PATH' | tee -a /etc/profile.d/slurm.sh

                # for armadillo, better to have it in the compute nodes
                yum groupinstall "Development Tools" -y
                yum install openblas-devel lapack-devel cmake -y
                wget ${ArmadilloUrl}
                tar -xvf armadillo-*.tar.xz
                rm -f armadillo-*.tar.xz
                cd armadillo-*
                cmake .
                make install


                # Launch Slurmd
                mkdir -p /var/spool/slurm
                sed "s|@SLURM_NODENAME@|cn${InstanceID}|" $SLURM_HOME/etc/slurm/slurmd.service > /lib/systemd/system/slurmd.service
                systemctl enable slurmd.service
                systemctl start slurmd.service
                                          
                /opt/aws/bin/cfn-signal -e $? --stack ${AWS::StackName} --resource ComputeNode${InstanceID} --region ${AWS::Region}
        CreationPolicy:
          ResourceSignal:
            Timeout: PT5M
        
  HeadNode:
    Type: AWS::EC2::Instance
#    Condition: Never
    Properties:
      ImageId: !Ref LatestAmiId
      InstanceType: !Ref HeadNodeInstanceType
      IamInstanceProfile: LabInstanceProfile
      KeyName:  !Ref KeyPair
      SecurityGroupIds:
        - !GetAtt [ SecurityGroup, GroupId ]
      AvailabilityZone: !Ref HeadNodeAZ
      Tags:
        - Key: Name
          Value: !Join [ "-", [ !Ref AWS::StackName, "headnode" ]]
      UserData: 
        Fn::Base64: !Sub |
              #!/bin/bash -x
              # Install packages
              yum update -y
              yum install nfs-utils -y
              yum install munge munge-libs munge-devel -y
              yum install openssl openssl-devel pam-devel numactl numactl-devel hwloc hwloc-devel lua lua-devel readline-devel rrdtool-devel ncurses-devel libibmad libibumad rpm-build pmix-devel -y
              yum groupinstall "Development Tools" -y

              yum install dbus-devel -y

              # Install OpenMPI
              yum install openmpi openmpi-devel eigen3 cmake -y
              
              # Configure NFS share
              mkdir -p /nfs
              echo "/nfs *(rw,async,no_subtree_check,no_root_squash)" | tee /etc/exports
              systemctl enable nfs-server
              systemctl start nfs-server
              exportfs -av
              
              # OpenMPI shared folder in NFS
              mkdir /nfs/mpi
              chmod 777 /nfs/mpi

              # Configure Munge
              echo "welcometoslurmamazonuserwelcometoslurmamazonuserwelcometoslurmamazonuser" | tee /etc/munge/munge.key
              chown munge:munge /etc/munge/munge.key
              chmod 600 /etc/munge/munge.key
              chown -R munge /etc/munge/ /var/log/munge/
              chmod 0700 /etc/munge/ /var/log/munge/
              systemctl enable munge
              systemctl start munge
              sleep 5

              # Install Slurm
              cd /home/ec2-user/
              wget -q ${SlurmPackageUrl}
              tar -xvf /home/ec2-user/slurm-*.tar.bz2 -C /home/ec2-user
              rm -f /home/ec2-user/slurm-*.tar.bz2
              cd /home/ec2-user/slurm-*
              /home/ec2-user/slurm-*/configure --prefix=/nfs/slurm --with-pmix
              make -j 4
              make install
              sleep 5
              export SLURM_HOME=/nfs/slurm
              mkdir -p $SLURM_HOME/etc/slurm
              'cp' /home/ec2-user/slurm-*/etc/* $SLURM_HOME/etc/slurm

              cat > $SLURM_HOME/etc/cgroup.conf <<'EOF'
              ConstrainCores=yes
              ConstrainDevices=yes
              ConstrainRAMSpace=yes
              ConstrainSwapSpace=yes
              EOF

              cat > $SLURM_HOME/etc/slurm.conf <<'EOF'
              ClusterName=${AWS::StackName}
              ControlMachine=@HEADNODE@
              ControlAddr=@HEADPRIVATEIP@
              SlurmdUser=root
              SlurmctldPort=6817
              SlurmdPort=6818
              AuthType=auth/munge
              StateSaveLocation=/var/spool/slurm/ctld
              SlurmdSpoolDir=/var/spool/slurm/d
              SwitchType=switch/none
              MpiDefault=none
              SlurmctldPidFile=/var/run/slurmctld.pid
              SlurmdPidFile=/var/run/slurmd.pid
              ProctrackType=proctrack/pgid
              returntoservice=2
              # TIMERS
              SlurmctldTimeout=300
              SlurmdTimeout=60
              InactiveLimit=0
              MinJobAge=300
              KillWait=30
              Waittime=0
              # SCHEDULING
              SchedulerType=sched/backfill
              SelectType=select/cons_tres
              SelectTypeParameters=CR_Core
              # LOGGING
              SlurmctldDebug=3
              SlurmctldLogFile=/var/log/slurmctld.log
              SlurmdDebug=3
              SlurmdLogFile=/var/log/slurmd.log
              DebugFlags=NO_CONF_HASH
              JobCompType=jobcomp/none
              # DYNAMIC COMPUTE NODES
              MaxNodeCount=8
              TreeWidth=65533
              PartitionName=aws Nodes=ALL Default=YES MaxTime=INFINITE State=UP
              EOF
              HOSTIP=`hostname -s | cut -c 4- | sed s'/-/./g'`
              sed -i -e "s|@HEADNODE@|$HOSTNAME|" -e "s|@HEADPRIVATEIP@|$HOSTIP|" $SLURM_HOME/etc/slurm.conf

              cat > $SLURM_HOME/etc/slurm/slurmd.service <<EOF
              [Unit]
              Description=Slurm node daemon
              After=munge.service network.target remote-fs.target
              [Service]
              Type=forking
              EnvironmentFile=-/etc/sysconfig/slurmd
              ExecStart=/nfs/slurm/sbin/slurmd -N @SLURM_NODENAME@ -Z -vv
              ExecReload=/bin/kill -HUP \$MAINPID
              PIDFile=/var/run/slurmd.pid
              KillMode=process
              LimitNOFILE=131072
              LimitMEMLOCK=infinity
              LimitSTACK=infinity
              Delegate=yes
              [Install]
              WantedBy=multi-user.target
              EOF

              cat > $SLURM_HOME/etc/slurm/slurmctld.service <<EOF
              [Unit]
              Description=Slurm controller daemon
              After=network.target munge.service
              ConditionPathExists=/nfs/slurm/etc/slurm.conf
              [Service]
              Type=forking
              EnvironmentFile=-/etc/sysconfig/slurmctld
              ExecStart=/nfs/slurm/sbin/slurmctld -vv
              ExecReload=/bin/kill -HUP \$MAINPID
              PIDFile=/var/run/slurmctld.pid
              LimitNOFILE=65536
              [Install]
              WantedBy=multi-user.target
              EOF

              # Set environment variables
              echo 'export SLURM_HOME=/nfs/slurm' | tee /etc/profile.d/slurm.sh
              echo 'export SLURM_CONF=$SLURM_HOME/etc/slurm.conf' | tee -a /etc/profile.d/slurm.sh
              echo 'export PATH=/nfs/slurm/bin:$PATH' | tee -a /etc/profile.d/slurm.sh

              # Launch Slurmctld
              mkdir -p /var/spool/slurm
              'cp' /nfs/slurm/etc/slurm/slurmd.service /lib/systemd/system
              'cp' /nfs/slurm/etc/slurm/slurmctld.service /lib/systemd/system
              systemctl enable slurmctld
              systemctl start slurmctld

              # Armadillo & dependencies
              yum install openblas-devel lapack-devel -y
              wget ${ArmadilloUrl}
              tar -xvf armadillo-*.tar.xz
              rm -f armadillo-*.tar.xz
              cd armadillo-*
              cmake .
              make install

              # Point cloud processing
              cd /nfs/mpi
              module load mpi
              wget ${PointCloudUrl}
              tar -xvf tarball
              rm -f tarball
              cd GarciaBarreiro-*
              wget https://lastools.github.io/download/LAStools_221128.zip
              unzip LAStools_221128.zip -d ./lib && rm -f LAStools_221128.zip
              (cd lib/LAStools && cmake . && make)
              (cmake -S . -B build -DCMAKE_BUILD_TYPE=Release -DCMAKE_PREFIX_PATH=/usr/lib64/openmpi/ && cd build && make)

              cd build

              # create a script to run the MPI program
              cat > sbatch.sh <<EOF
              #!/bin/bash
              #SBATCH -o %x-%J.out
              #SBATCH -t 00:20:00
              #SBATCH --ntasks-per-node=1
              #SBATCH --exclusive

              mpirun ./rule-based-classifier-cpp -i ../data/ptR_18C.las -r 2
              EOF

              chown -R ec2-user /nfs/mpi

              /opt/aws/bin/cfn-signal -e $? --stack ${AWS::StackName} --resource HeadNode --region ${AWS::Region}
    CreationPolicy:
      ResourceSignal:
        Timeout: PT10M
        
Outputs:
  HeadNodeId:
    Description: Head node instance ID
    Value: !Ref HeadNode
#    Condition: Never
